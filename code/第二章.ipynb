{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 通过API调用模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好，CAMEL AI，这是一个致力于研究自主交互式代理的开源社区。\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n===============================================================================\\n你好，向CAMEL AI这个致力于自主交互式智能体研究的开源社区问好。\\n===============================================================================\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from camel.agents import ChatAgent\n",
    "from camel.configs import ZhipuAIConfig\n",
    "from camel.messages import BaseMessage\n",
    "from camel.models import ModelFactory\n",
    "from camel.types import ModelPlatformType, ModelType\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "model = ModelFactory.create(\n",
    "    model_platform=ModelPlatformType.ZHIPU,\n",
    "    model_type=ModelType.GLM_4,\n",
    "    model_config_dict=ZhipuAIConfig(temperature=0.2).as_dict(),\n",
    "    api_key=os.environ.get(\"ZHIPUAI_API_KEY\"),\n",
    "    url=os.environ.get(\"ZHIPUAI_API_BASE_URL\"),\n",
    ")\n",
    "\n",
    "# 设置system prompt\n",
    "sys_msg = BaseMessage.make_assistant_message(\n",
    "    role_name=\"Assistant\",\n",
    "    content=\"You are a helpful assistant.\",\n",
    ")\n",
    "\n",
    "# 初始化agent\n",
    "camel_agent = ChatAgent(system_message=sys_msg, model=model, output_language=\"zh\")#这里同样可以设置输出语言\n",
    "\n",
    "user_msg = BaseMessage.make_user_message(\n",
    "    role_name=\"User\",\n",
    "    content=\"\"\"Say hi to CAMEL AI, one open-source community \n",
    "    dedicated to the study of autonomous and communicative agents.\"\"\",\n",
    ")\n",
    "\n",
    "\n",
    "# 调用模型\n",
    "response = camel_agent.step(user_msg)\n",
    "print(response.msgs[0].content)\n",
    "\n",
    "#以下是模型回复的内容\n",
    "'''\n",
    "===============================================================================\n",
    "你好，向CAMEL AI这个致力于自主交互式智能体研究的开源社区问好。\n",
    "===============================================================================\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelFactory.create(\n",
    "    model_platform=ModelPlatformType.OPENAI_COMPATIBLE_MODEL,\n",
    "    model_type=\"a-string-representing-the-model-type\",\n",
    "    api_key=os.environ.get(\"OPENAI_COMPATIBILIY_API_KEY\"),\n",
    "    url=os.environ.get(\"OPENAI_COMPATIBILIY_API_BASE_URL\"),\n",
    "    model_config_dict={\"temperature\": 0.4, \"max_tokens\": 4096},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BaseMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseMessage(role_name='example_user', role_type=<RoleType.USER: 'user'>, meta_dict={}, content='Hello, CAMEL!', video_bytes=None, image_list=None, image_detail='auto', video_detail='low', parsed=None)\n"
     ]
    }
   ],
   "source": [
    "from camel.messages import BaseMessage\n",
    "from camel.types import RoleType\n",
    "\n",
    "# 创建一个简单的用户消息\n",
    "message = BaseMessage(\n",
    "    role_name=\"example_user\",\n",
    "    role_type=RoleType.USER,\n",
    "    content=\"Hello, CAMEL!\",\n",
    "    meta_dict={} #添加必需的meta dict参数，即使为空也要提供，否则会报 TypeError\n",
    ")\n",
    "\n",
    "print(message)\n",
    "\n",
    "# >>> BaseMessage(role_name='example_user', role_type=<RoleType.USER: 'user'>, meta_dict={}, content='Hello, CAMEL!', video_bytes=None, image_list=None, image_detail='auto', video_detail='low', parsed=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseMessage(role_name='User_with_image', role_type=<RoleType.USER: 'user'>, meta_dict={}, content='Here is an image', video_bytes=None, image_list=[<PIL.PngImagePlugin.PngImageFile image mode=RGBA size=3520x720 at 0x7F5E0AF78220>], image_detail='auto', video_detail='low', parsed=None)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import requests\n",
    "\n",
    "# 下载一张图片并创建一个 PIL Image 对象\n",
    "url = \"https://raw.githubusercontent.com/camel-ai/camel/master/misc/logo_light.png\"\n",
    "response = requests.get(url)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "\n",
    "# 创建包含图片的用户消息\n",
    "image_message = BaseMessage(\n",
    "    role_name=\"User_with_image\",\n",
    "    role_type=RoleType.USER,\n",
    "    content=\"Here is an image\",\n",
    "    meta_dict={},\n",
    "    image_list=[img]  # 将图片列表作为参数传入\n",
    ")\n",
    "\n",
    "print(image_message)\n",
    "\n",
    "# >>> BaseMessage(role_name='User_with_image', role_type=<RoleType.USER: 'user'>, meta_dict={}, content='Here is an image', video_bytes=None, image_list=[<PIL.PngImagePlugin.PngImageFile image mode=RGBA size=3520x720 at 0x1DDFF8E88F0>], image_detail='auto', video_detail='low', parsed=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Message: BaseMessage(role_name='User_1', role_type=<RoleType.USER: 'user'>, meta_dict=None, content='Hi, what can you do?', video_bytes=None, image_list=None, image_detail='auto', video_detail='low', parsed=None)\n",
      "Assistant Message: BaseMessage(role_name='Assistant_1', role_type=<RoleType.ASSISTANT: 'assistant'>, meta_dict=None, content='I can help you with various tasks.', video_bytes=None, image_list=None, image_detail='auto', video_detail='low', parsed=None)\n"
     ]
    }
   ],
   "source": [
    "from camel.messages import BaseMessage\n",
    "\n",
    "# 创建用户消息\n",
    "user_msg = BaseMessage.make_user_message(\n",
    "    role_name=\"User_1\",\n",
    "    content=\"Hi, what can you do?\"\n",
    ")\n",
    "\n",
    "# 创建助手消息\n",
    "assistant_msg = BaseMessage.make_assistant_message(\n",
    "    role_name=\"Assistant_1\",\n",
    "    content=\"I can help you with various tasks.\"\n",
    ")\n",
    "\n",
    "print(\"User Message:\", user_msg)\n",
    "print(\"Assistant Message:\", assistant_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated User Message: BaseMessage(role_name='User_1', role_type=<RoleType.USER: 'user'>, meta_dict=None, content='Hi, can you tell me more about CAMEL?', video_bytes=None, image_list=None, image_detail='auto', video_detail='low', parsed=None)\n"
     ]
    }
   ],
   "source": [
    "# 基于用户消息创建一个新消息，内容稍作修改\n",
    "updated_user_msg = user_msg.create_new_instance(\"Hi, can you tell me more about CAMEL?\")\n",
    "print(\"Updated User Message:\", updated_user_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message as dict: {'role_name': 'Assistant_1', 'role_type': 'ASSISTANT', 'content': 'I can help you with various tasks.'}\n"
     ]
    }
   ],
   "source": [
    "msg_dict = assistant_msg.to_dict()\n",
    "print(\"Message as dict:\", msg_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI-compatible user message: {'role': 'user', 'content': 'Hi, what can you do?'}\n",
      "OpenAI-compatible assistant message: {'role': 'assistant', 'content': 'I can help you with various tasks.'}\n"
     ]
    }
   ],
   "source": [
    "from camel.types import OpenAIBackendRole\n",
    "\n",
    "# 将用户消息转化为OpenAI后端兼容的用户消息\n",
    "openai_user_msg = user_msg.to_openai_message(role_at_backend=OpenAIBackendRole.USER)\n",
    "print(\"OpenAI-compatible user message:\", openai_user_msg)\n",
    "\n",
    "# 将助手消息转化为OpenAI后端的助手消息\n",
    "openai_assistant_msg = assistant_msg.to_openai_assistant_message()\n",
    "print(\"OpenAI-compatible assistant message:\", openai_assistant_msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 与ChatAgent协作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant Response: 当然可以！CAMEL AI 是一个基于语言模型的智能助手，设计用于进行多轮对话，能够理解、生成自然语言，并执行各种任务。CAMEL 代表“ Cooperative Agents for Multimodal Efficient Learning”，即多模态高效学习的合作代理。这个系统旨在通过合作的方式，使多个AI代理能够共同完成复杂的任务，提高解决问题的效率和质量。CAMEL AI 可以应用于多种场景，比如客户服务、教育辅导、信息查询等，为用户提供便捷、高效的交互体验。\n"
     ]
    }
   ],
   "source": [
    "from camel.agents import ChatAgent\n",
    "from camel.models import ModelFactory\n",
    "from camel.types import ModelPlatformType\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv('QWEN_API_KEY')\n",
    "\n",
    "model = ModelFactory.create(\n",
    "    model_platform=ModelPlatformType.OPENAI_COMPATIBLE_MODEL,\n",
    "    model_type=\"Qwen/Qwen2.5-72B-Instruct\",\n",
    "    url='https://api-inference.modelscope.cn/v1/',\n",
    "    api_key=api_key\n",
    ")\n",
    "\n",
    "# 创建系统消息，告诉ChatAgent自己的角色定位\n",
    "system_msg = \"You are a helpful assistant that responds to user queries.\"\n",
    "\n",
    "# 实例化一个ChatAgent\n",
    "chat_agent = ChatAgent(model=model, system_message=system_msg,output_language='zh')\n",
    "\n",
    "# 构造用户消息\n",
    "user_msg = \"Hello! Can you tell me something about CAMEL AI?\"\n",
    "\n",
    "# 将用户消息传给ChatAgent，并获取回复\n",
    "response = chat_agent.step(user_msg)\n",
    "print(\"Assistant Response:\", response.msgs[0].content)\n",
    "\n",
    "# >>> Assistant Response: 当然可以！CAMEL AI 是一个先进的语言模型，它被设计用来进行多轮对话、理解复杂指令并生成高质量的文本。这个模型能够处理各种任务，比如回答问题、撰写文章、创作故事等。CAMEL AI 的目标是通过自然语言处理技术，为用户提供更加智能和人性化的交互体验。如果你有任何具体的问题或需要帮助的地方，欢迎随时告诉我！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant Response with metadata: 好的，请提供具体的元数据内容，这样我可以更好地理解和回应您的需求。如果您有任何特定的问题或需要进一步的信息，请告诉我！\n"
     ]
    }
   ],
   "source": [
    "# 在用户消息中添加元数据\n",
    "user_msg_with_meta = BaseMessage.make_user_message(\n",
    "    role_name=\"User\",\n",
    "    content=\"Here is some extra context in the metadata.\",\n",
    "    meta_dict={\"context_info\": \"User is interested in AI frameworks.\"}\n",
    ")\n",
    "\n",
    "response_with_meta = chat_agent.step(user_msg_with_meta)\n",
    "print(\"Assistant Response with metadata:\", response_with_meta.msgs[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-30 21:04:24,551 - root - WARNING - Invalid or missing `max_tokens` in `model_config_dict`. Defaulting to 999_999_999 tokens.\n",
      "Assistant's description of the image: 这是一张金毛寻回犬的特写照片。这只狗有着浓密的金色毛发，耳朵垂在头部两侧，眼睛明亮而有神，黑色的鼻子显得很突出。它的嘴巴微微张开，露出了粉红色的舌头，看起来非常友好和活泼。背景是一片模糊的绿色，可能是在户外的自然环境中拍摄的。整体来说，这张照片展示了一只健康、快乐的金毛寻回犬，它的表情充满了热情和活力。\n"
     ]
    }
   ],
   "source": [
    "from camel.agents import ChatAgent\n",
    "from camel.messages import BaseMessage\n",
    "from camel.models import ModelFactory\n",
    "from camel.types import ModelPlatformType,RoleType\n",
    "\n",
    "from io import BytesIO\n",
    "import requests\n",
    "from PIL import Image\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv('QWEN_API_KEY')\n",
    "\n",
    "model = ModelFactory.create(\n",
    "    model_platform=ModelPlatformType.OPENAI_COMPATIBLE_MODEL,\n",
    "    model_type=\"Qwen/QVQ-72B-Preview\",\n",
    "    url='https://api-inference.modelscope.cn/v1/',\n",
    "    api_key=api_key\n",
    ")\n",
    "\n",
    "# 实例化ChatAgent\n",
    "chat_agent = ChatAgent(model=model,output_language='中文')\n",
    "\n",
    "# 图片URL\n",
    "url = \"https://img0.baidu.com/it/u=2205376118,3235587920&fm=253&fmt=auto&app=120&f=JPEG?w=846&h=800\"\n",
    "response = requests.get(url)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "\n",
    "user_image_msg = BaseMessage.make_user_message(\n",
    "    role_name=\"User\", \n",
    "    content=\"请描述这张图片的内容\", \n",
    "    image_list=[img]  # 将图片放入列表中\n",
    ")\n",
    "\n",
    "# 将包含图片的消息传给ChatAgent\n",
    "response_with_image = chat_agent.step(user_image_msg)\n",
    "print(\"Assistant's description of the image:\", response_with_image.msgs[0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "消息内容: 你好，我可以帮您做什么？\n",
      "会话是否终止: False\n",
      "附加信息: {'usage': {'prompt_tokens': 10, 'completion_tokens': 15}}\n"
     ]
    }
   ],
   "source": [
    "from camel.responses import ChatAgentResponse\n",
    "from camel.messages import BaseMessage\n",
    "from camel.types import RoleType\n",
    "\n",
    "# 创建一个 ChatAgentResponse 实例\n",
    "response = ChatAgentResponse(\n",
    "    msgs=[\n",
    "        BaseMessage(\n",
    "            role_name=\"Assistant\",  # 助手的角色名称\n",
    "            role_type=RoleType.ASSISTANT,  # 指定角色类型\n",
    "            content=\"你好，我可以帮您做什么？\",  # 消息内容\n",
    "            meta_dict={}  # 提供一个空的元数据字典（可根据需要填充）\n",
    "        )\n",
    "    ],  \n",
    "    terminated=False,  # 会话未终止\n",
    "    info={\"usage\": {\"prompt_tokens\": 10, \"completion_tokens\": 15}}  # 附加信息\n",
    ")\n",
    "\n",
    "# 访问属性\n",
    "messages = response.msgs  # 获取Agent生成的消息\n",
    "is_terminated = response.terminated  # 会话是否终止\n",
    "additional_info = response.info  # 获取附加信息\n",
    "\n",
    "# 打印消息内容\n",
    "print(\"消息内容:\", messages[0].content)\n",
    "# 打印会话是否终止\n",
    "print(\"会话是否终止:\", is_terminated)\n",
    "# 打印附加信息\n",
    "print(\"附加信息:\", additional_info)\n",
    "\n",
    "# >>> 消息内容: 你好，我可以帮您做什么？\n",
    "# >>> 会话是否终止: False\n",
    "# >>> 附加信息: {'usage': {'prompt_tokens': 10, 'completion_tokens': 15}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specified task prompt:\n",
      "音乐家将指导学生如何在舞台上自信地移动，掌握与观众互动的技巧，以及如何通过面部表情和肢体语言传达情感，以提升整体表演魅力。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from camel.agents import TaskSpecifyAgent\n",
    "from camel.models import ModelFactory\n",
    "from camel.types import ModelPlatformType, TaskType\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv('QWEN_API_KEY')\n",
    "\n",
    "model = ModelFactory.create(\n",
    "    model_platform=ModelPlatformType.OPENAI_COMPATIBLE_MODEL,\n",
    "    model_type=\"Qwen/Qwen2.5-72B-Instruct\",\n",
    "    url='https://api-inference.modelscope.cn/v1/',\n",
    "    api_key=api_key\n",
    ")\n",
    "task_specify_agent = TaskSpecifyAgent(\n",
    "    model=model, task_type=TaskType.AI_SOCIETY,output_language='中文'\n",
    ")\n",
    "specified_task_prompt = task_specify_agent.run(\n",
    "    task_prompt=\"Improving stage presence and performance skills\",\n",
    "    meta_dict=dict(\n",
    "        assistant_role=\"Musician\", user_role=\"Student\", word_limit=100\n",
    "    ),\n",
    ")\n",
    "print(f\"Specified task prompt:\\n{specified_task_prompt}\\n\")\n",
    "\n",
    "# >>>\n",
    "# Specified task prompt:\n",
    "# Musician will coach Student on dynamic stage movement, audience engagement techniques, and emotional expression during performances, enhancing overall charisma and connection with the audience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! To make your goal of getting a promotion as a Software Engineer more specific, you can break it down into clear, actionable steps. Here’s a more detailed version of your task:\n",
      "\n",
      "### Goal: Get Promoted to [Next Position, e.g., Senior Software Engineer, Lead Developer, etc.]\n",
      "\n",
      "#### 1. **Understand the Requirements**\n",
      "   - **Research the Role:** Identify the specific role you are aiming for (e.g., Senior Software Engineer, Lead Developer).\n",
      "   - **Review Job Descriptions:** Look at job postings for the position to understand the required skills, experience, and responsibilities.\n",
      "   - **Talk to Your Manager:** Schedule a meeting with your manager to discuss the criteria for promotion and any areas where you need to improve.\n",
      "\n",
      "#### 2. **Assess Your Current Skills and Experience**\n",
      "   - **Self-Evaluation:** Evaluate your current skills and experience against the requirements of the next role.\n",
      "   - **Seek Feedback:** Ask for feedback from your manager, peers, and mentors to identify your strengths and areas for improvement.\n",
      "\n",
      "#### 3. **Develop a Plan**\n",
      "   - **Skill Development:** Create a plan to develop the necessary skills. This might include:\n",
      "     - **Online Courses:** Enroll in relevant courses (e.g., advanced programming, leadership, project management).\n",
      "     - **Certifications:** Obtain relevant certifications (e.g., AWS Certified Solutions Architect, Scrum Master).\n",
      "     - **Projects:** Take on additional projects or responsibilities that align with the next role.\n",
      "   - **Networking:** Build relationships with key stakeholders and leaders within the company.\n",
      "   - **Mentorship:** Find a mentor who has already achieved the position you are aiming for.\n",
      "\n",
      "#### 4. **Demonstrate Your Value**\n",
      "   - **Exceed Expectations:** Consistently deliver high-quality work and exceed expectations in your current role.\n",
      "   - **Take Initiative:** Volunteer for challenging projects and take on leadership roles in team initiatives.\n",
      "   - **Document Achievements:** Keep a record of your accomplishments, including successful projects, awards, and positive feedback.\n",
      "\n",
      "#### 5. **Prepare for the Promotion Process**\n",
      "   - **Update Your Resume and Portfolio:** Ensure your resume and portfolio reflect your most recent achievements and the skills you have developed.\n",
      "   - **Prepare for Interviews:** If the promotion process includes interviews, practice common interview questions and prepare examples of your achievements.\n",
      "   - **Leverage Connections:** Use your network to advocate for your promotion.\n",
      "\n",
      "#### 6. **Request the Promotion**\n",
      "   - **Schedule a Meeting:** Request a formal meeting with your manager to discuss your readiness for the next role.\n",
      "   - **Present Your Case:** Clearly articulate your achievements, the skills you have developed, and why you believe you are ready for the promotion.\n",
      "   - **Be Open to Feedback:** Be prepared to receive feedback and address any concerns your manager may have.\n",
      "\n",
      "#### 7. **Follow Up and Reflect**\n",
      "   - **Follow Up:** After the meeting, send a thank-you note and reiterate your interest in the position.\n",
      "   - **Reflect and Adjust:** Regardless of the outcome, reflect on the process and adjust your plan as needed. If you don’t get the promotion, ask for specific feedback and use it to improve.\n",
      "\n",
      "By breaking down your goal into these specific steps, you can create a clear and actionable plan to increase your chances of getting promoted. Good luck!\n"
     ]
    }
   ],
   "source": [
    "from camel.agents import TaskSpecifyAgent\n",
    "from camel.models import ModelFactory\n",
    "from camel.prompts import TextPrompt\n",
    "from camel.types import ModelPlatformType\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv('QWEN_API_KEY')\n",
    "\n",
    "model = ModelFactory.create(\n",
    "    model_platform=ModelPlatformType.OPENAI_COMPATIBLE_MODEL,\n",
    "    model_type=\"Qwen/Qwen2.5-72B-Instruct\",\n",
    "    url='https://api-inference.modelscope.cn/v1/',\n",
    "    api_key=api_key\n",
    ")\n",
    "\n",
    "my_prompt_template = TextPrompt(\n",
    "    'Here is a task: I\\'m a {occupation} and I want to {task}. Help me to make this task more specific.'\n",
    ")  # 你可以根据需求自定义任何模板\n",
    "task_specify_agent = TaskSpecifyAgent(\n",
    "    model=model, task_specify_prompt=my_prompt_template\n",
    ")\n",
    "response = task_specify_agent.run(\n",
    "    task_prompt=\"get promotion\",\n",
    "    meta_dict=dict(occupation=\"Software Engineer\"),\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter your name and age: {name}, {age}\n"
     ]
    }
   ],
   "source": [
    "from camel.prompts import TextPrompt\n",
    "prompt = TextPrompt('Please enter your name and age: {name}, {age}')\n",
    "print(prompt)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name', 'age'}\n"
     ]
    }
   ],
   "source": [
    "from camel.prompts import TextPrompt\n",
    "prompt = TextPrompt('Please enter your name and age: {name}, {age}')\n",
    "print(prompt.key_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your name and age are: John, 30\n"
     ]
    }
   ],
   "source": [
    "from camel.prompts import TextPrompt\n",
    "prompt = TextPrompt('Your name and age are: {name}, {age}')\n",
    "name, age = 'John', 30\n",
    "formatted_prompt = prompt.format(name=name, age=age)\n",
    "print(formatted_prompt)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your name and age are: John, {age}\n"
     ]
    }
   ],
   "source": [
    "from camel.prompts import TextPrompt\n",
    "prompt = TextPrompt('Your name and age are: {name}, {age}')\n",
    "name = 'John'\n",
    "partial_formatted_prompt = prompt.format(name=name)\n",
    "print(partial_formatted_prompt)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, {name}! Welcome, {name}!\n",
      "True\n",
      "{'name'}\n",
      "Hello, {name}! Welcome, {name}!\n",
      "True\n",
      "{'name'}\n",
      "HELLO, {NAME}! WELCOME, {NAME}!\n",
      "True\n",
      "{'NAME'}\n"
     ]
    }
   ],
   "source": [
    "from camel.prompts import TextPrompt\n",
    "prompt1 = TextPrompt('Hello, {name}!')\n",
    "prompt2 = TextPrompt('Welcome, {name}!')\n",
    "#连接\n",
    "prompt3 = prompt1 + ' ' + prompt2\n",
    "print(prompt3)  \n",
    "# >>> \"Hello, {name}! Welcome, {name}!\"\n",
    "print(isinstance(prompt3, TextPrompt))\n",
    "# >>> True\n",
    "print(prompt3.key_words)\n",
    "# >>> {'name'}\n",
    "#连接\n",
    "prompt4 = TextPrompt(' ').join([prompt1, prompt2])\n",
    "print(prompt4)\n",
    "# >>> \"Hello, {name}! Welcome, {name}!\"\n",
    "print(isinstance(prompt4, TextPrompt))\n",
    "# >>> True\n",
    "print(prompt4.key_words)\n",
    "# >>> {'name'}\n",
    "#应用字符串方法\n",
    "prompt5 = prompt4.upper()\n",
    "print(prompt5)\n",
    "# >>> \"HELLO, {NAME}! WELCOME, {NAME}!\"\n",
    "print(isinstance(prompt5, TextPrompt))\n",
    "# >>> True\n",
    "print(prompt5.key_words)\n",
    "# >>> {'NAME'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "消息: Hello,今天感觉怎么样？, 权重: 0.40960000000000013\n",
      "消息: 我很好，谢谢！, 权重: 0.5120000000000001\n",
      "消息: 你能做些什么？, 权重: 0.6400000000000001\n",
      "消息: 我可以帮助你完成各种任务。, 权重: 0.8\n"
     ]
    }
   ],
   "source": [
    "from camel.memories.blocks import ChatHistoryBlock\n",
    "from camel.memories.records import MemoryRecord\n",
    "from camel.types import OpenAIBackendRole\n",
    "from camel.messages import BaseMessage\n",
    "\n",
    "# 创建一个 ChatHistoryBlock 实例\n",
    "chat_history = ChatHistoryBlock(keep_rate=0.8)\n",
    "\n",
    "# 模拟写入一些消息记录\n",
    "chat_history.write_records([\n",
    "    MemoryRecord(message=BaseMessage.make_assistant_message(role_name=\"user\", content=\"Hello,今天感觉怎么样？\"), role_at_backend=OpenAIBackendRole.USER),\n",
    "    MemoryRecord(message=BaseMessage.make_user_message(role_name=\"assistant\", content=\"我很好，谢谢！\"), role_at_backend=OpenAIBackendRole.ASSISTANT),\n",
    "    MemoryRecord(message=BaseMessage.make_user_message(role_name=\"user\", content=\"你能做些什么？\"), role_at_backend=OpenAIBackendRole.USER),\n",
    "    MemoryRecord(message=BaseMessage.make_assistant_message(role_name=\"assistant\", content=\"我可以帮助你完成各种任务。\"), role_at_backend=OpenAIBackendRole.ASSISTANT),\n",
    "])\n",
    "\n",
    "# 检索最近的 3 条消息\n",
    "recent_records = chat_history.retrieve(window_size=4)\n",
    "\n",
    "for record in recent_records:\n",
    "    print(f\"消息: {record.memory_record.message.content}, 权重: {record.score}\")\n",
    "    \n",
    "# >>>\n",
    "# 消息: hello,你怎么样？, 权重: 0.40960000000000013\n",
    "# 消息: 我很好，谢谢！, 权重: 0.5120000000000001\n",
    "# 消息: 你能做些什么？, 权重: 0.6400000000000001\n",
    "# 消息: 我可以帮助你完成各种任务。, 权重: 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from camel.memories.blocks.vectordb_block import VectorDBBlock\n",
    "from camel.memories.records import MemoryRecord\n",
    "from camel.messages import BaseMessage\n",
    "from camel.embeddings import SentenceTransformerEncoder\n",
    "from camel.types import OpenAIBackendRole\n",
    "\n",
    "# 创建一个 VectorDBBlock 实例\n",
    "vector_db_block = VectorDBBlock(embedding=SentenceTransformerEncoder(model_name=\"BAAI/bge-m3\"))\n",
    "\n",
    "# 创建一些示例聊天记录\n",
    "records = [\n",
    "    MemoryRecord(message=BaseMessage.make_user_message(role_name=\"user\", content=\"今天天气真好！\"), role_at_backend=OpenAIBackendRole.USER),\n",
    "    MemoryRecord(message=BaseMessage.make_user_message(role_name=\"user\", content=\"你喜欢什么运动？\"), role_at_backend=OpenAIBackendRole.USER),\n",
    "    MemoryRecord(message=BaseMessage.make_user_message(role_name=\"user\", content=\"今天天气不错，我们去散步吧。\"), role_at_backend=OpenAIBackendRole.USER),\n",
    "]\n",
    "\n",
    "# 将记录写入向量数据库\n",
    "vector_db_block.write_records(records)\n",
    "\n",
    "# 使用关键词进行检索\n",
    "keyword = \"天气\"\n",
    "retrieved_records = vector_db_block.retrieve(keyword=keyword, limit=3)\n",
    "\n",
    "for record in retrieved_records:\n",
    "    print(f\"UUID: {record.memory_record.uuid}, Message: {record.memory_record.message.content}, Score: {record.score}\")\n",
    "# >>>\n",
    "# UUID: f7519828-afe7-41e4-8331-7bbc4d7dcbd5, Message: 今天天气真好！, Score: 0.779863416845349\n",
    "# UUID: 0fbab391-f0e0-4580-877b-8fa2a837675b, Message: 今天天气不错，我们去散步吧。, Score: 0.6920892191464151\n",
    "# UUID: a640cf33-987b-4b52-ac2b-a987dd474e4e, Message: 你喜欢什么运动？, Score: 0.534536213348924"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/camel/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'user', 'content': '什么是CAMEL AI?'}, {'role': 'assistant', 'content': 'CAMEL-AI是第一个LLM多智能体框架,并且是一个致力于寻找智能体 scaling law 的开源社区。'}]\n",
      "token消耗: 52\n"
     ]
    }
   ],
   "source": [
    "from camel.memories import (\n",
    "    LongtermAgentMemory,\n",
    "    MemoryRecord,\n",
    "    ScoreBasedContextCreator,\n",
    "    ChatHistoryBlock,\n",
    "    VectorDBBlock,\n",
    ")\n",
    "from camel.messages import BaseMessage\n",
    "from camel.types import ModelType, OpenAIBackendRole\n",
    "from camel.utils import OpenAITokenCounter\n",
    "from camel.embeddings import SentenceTransformerEncoder\n",
    "\n",
    "# 1. 初始化内存系统\n",
    "memory = LongtermAgentMemory(\n",
    "    context_creator=ScoreBasedContextCreator(\n",
    "        token_counter=OpenAITokenCounter(ModelType.GPT_4O_MINI),\n",
    "        token_limit=1024,\n",
    "    ),\n",
    "    chat_history_block=ChatHistoryBlock(),\n",
    "    vector_db_block=VectorDBBlock(embedding=SentenceTransformerEncoder(model_name=\"BAAI/bge-m3\")),\n",
    ")\n",
    "\n",
    "# 2. 创建记忆记录\n",
    "records = [\n",
    "    MemoryRecord(\n",
    "        message=BaseMessage.make_user_message(\n",
    "            role_name=\"User\",\n",
    "            content=\"什么是CAMEL AI?\"\n",
    "        ),\n",
    "        role_at_backend=OpenAIBackendRole.USER,\n",
    "    ),\n",
    "    MemoryRecord(\n",
    "        message=BaseMessage.make_assistant_message(\n",
    "            role_name=\"Agent\",\n",
    "            content=\"CAMEL-AI是第一个LLM多智能体框架,并且是一个致力于寻找智能体 scaling law 的开源社区。\"\n",
    "        ),\n",
    "        role_at_backend=OpenAIBackendRole.ASSISTANT,\n",
    "    ),\n",
    "]\n",
    "\n",
    "# 3. 写入记忆\n",
    "memory.write_records(records)\n",
    "\n",
    "context, token_count = memory.get_context()\n",
    "\n",
    "print(context)\n",
    "print(f'token消耗: {token_count}')\n",
    "\n",
    "# >>>\n",
    "# [{'role': 'user', 'content': '什么是CAMEL AI?'}, {'role': 'assistant', 'content': 'CAMEL-AI是第一个LLM多智能体框架,并且是一个致力于寻找智能体 scaling law 的开源社区。'}]\n",
    "# token消耗: 52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "目前，关于“第一个LLM（大语言模型）多智能体框架”的具体定义可能还存在一些模糊性，因为这一领域正在快速发展中，而且不同的研究团队和公司可能会有不同的理解和实现方式。不过，可以提到几个在这一领域内较早进行探索的项目或平台：\n",
      "\n",
      "1. **Multi-Agent Reinforcement Learning (MARL)**：虽然严格来说MARL并不是专门为大语言模型设计的，但它确实是一个早期尝试将多个智能体结合起来解决问题的框架，这些智能体可以通过强化学习来优化它们的行为策略。随着大语言模型的发展，有些研究开始尝试将大语言模型集成到MARL框架中，以增强智能体之间的沟通和协作能力。\n",
      "\n",
      "2. **OpenAI的MicroWorlds**：这是一个更接近于“LLM多智能体”概念的例子。在这个环境中，多个由大语言模型驱动的智能体可以在一个模拟的世界里互动，执行任务，甚至相互学习。尽管这可能不是最早的概念，但它代表了将大语言模型应用于多智能体系统的一种创新方法。\n",
      "\n",
      "3. **Anthropic的Constitutional AI**：虽然这个项目更多地关注于通过一组原则指导AI行为，而不是直接创建一个多智能体系统，但其理念对于构建能够相互协作的大语言模型智能体网络有着重要的启示作用。\n",
      "\n",
      "需要注意的是，由于该领域正处于快速发展的阶段，新的框架和技术不断涌现，因此很难明确指出哪一个就是“第一个”。如果你有特定的研究方向或者应用场景，或许可以提供更详细的信息，这样我就能给出更加准确的答案了。\n"
     ]
    }
   ],
   "source": [
    "from camel.agents import ChatAgent\n",
    "from camel.models import ModelFactory\n",
    "from camel.types import ModelPlatformType\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "# 定义系统消息\n",
    "sys_msg = \"你是一个好奇的智能体，正在探索宇宙的奥秘。\"\n",
    "\n",
    "# 初始化agent 调用在线Qwen/Qwen2.5-72B-Instruct\n",
    "api_key = os.getenv('QWEN_API_KEY')\n",
    "\n",
    "model = ModelFactory.create(\n",
    "    model_platform=ModelPlatformType.OPENAI_COMPATIBLE_MODEL,\n",
    "    model_type=\"Qwen/Qwen2.5-72B-Instruct\",\n",
    "    url='https://api-inference.modelscope.cn/v1/',\n",
    "    api_key=api_key\n",
    ")\n",
    "agent = ChatAgent(system_message=sys_msg, model=model)\n",
    "\n",
    "# 定义用户消息\n",
    "usr_msg = \"告诉我基于我们讨论的内容，哪个是第一个LLM多智能体框架？\"\n",
    "\n",
    "# 发送消息给agent\n",
    "response = agent.step(usr_msg)\n",
    "\n",
    "# 查看响应\n",
    "print(response.msgs[0].content)\n",
    "\n",
    "# >>>\n",
    "# \"截至目前（2023年），还没有一个明确被广泛认可为“第一个”LLM多智能体框架的具体项目或平台。然而，随着大语言模型（LLMs）和多智能体系统研 \n",
    "# 究的快速发展，一些早期尝试将这两者结合起来的项目可以被视为先驱。\n",
    "\n",
    "# 例如，“AgentVerse”是一个基于大语言模型的多智能体框架，它允许不同的AI代理相互交互以完成任务或解决问题。虽然不能确切地说它是第一个，但\n",
    "# 它确实是在这一领域较早进行探索的一个例子。\n",
    "\n",
    "# 此外，还有其他一些研究和项目也在探索如何利用LLMs构建多智能体系统，比如通过模拟社会动态、协作解决问题等。这些工作都在不同程度上推动了\n",
    "# 该领域的进步。\n",
    "\n",
    "# 如果您有更具体的时间范围或其他标准来定义“第一个”，请提供更多信息，这样我可以尝试给出更加准确的答案。\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "根据我们之前的讨论，CAMEL-AI 是被指认为第一个LLM（大型语言模型）多智能体框架的项目。这个框架旨在通过多智能体系统来探索和实现更复杂、更高效的AI协作与交互模式。\n"
     ]
    }
   ],
   "source": [
    "# 将memory赋值给agent\n",
    "agent.memory = memory\n",
    "# 发送消息给agent\n",
    "response = agent.step(usr_msg)\n",
    "# 查看响应\n",
    "print(response.msgs[0].content)\n",
    "# >>>\n",
    "# \"基于我们讨论的内容，CAMEL-AI 是第一个基于大型语言模型（LLM）的多智能体框架。\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from camel.agents import ChatAgent\n",
    "from camel.models import ModelFactory\n",
    "from camel.types import ModelPlatformType\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import math\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# 定义系统消息\n",
    "sys_msg = \"你是一个数学大师，擅长各种数学问题。\"\n",
    "\n",
    "# 初始化agent\n",
    "api_key = os.getenv('QWEN_API_KEY')\n",
    "\n",
    "model = ModelFactory.create(\n",
    "    model_platform=ModelPlatformType.OPENAI_COMPATIBLE_MODEL,\n",
    "    model_type=\"Qwen/Qwen2.5-72B-Instruct\",\n",
    "    url='https://api-inference.modelscope.cn/v1/',\n",
    "    api_key=api_key\n",
    ")\n",
    "\n",
    "# 创建agent并添加工具\n",
    "agent = ChatAgent(\n",
    "    system_message=sys_msg,\n",
    "    model=model,\n",
    "    output_language='中文',\n",
    "    tools=[]\n",
    ")\n",
    "\n",
    "# 定义用户消息\n",
    "usr_msg = \"2的平方根是多少？\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2的平方根是 \\(\\sqrt{2}\\)，其数值约为1.414。\n"
     ]
    }
   ],
   "source": [
    "# 发送消息给agent\n",
    "response = agent.step(usr_msg)\n",
    "print(response.msgs[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from camel.toolkits import FunctionTool\n",
    "import math\n",
    "\n",
    "def calculate_sqrt(x: float) -> float:\n",
    "    r\"\"\"计算一个数的平方根。\n",
    "\n",
    "    Args:\n",
    "        x (float): 需要计算平方根的数字。\n",
    "\n",
    "    Returns:\n",
    "        float: 输入数字的平方根。\n",
    "    \"\"\"\n",
    "    return math.sqrt(x)\n",
    "\n",
    "# 用 FunctionTool 包装该函数\n",
    "add_tool = FunctionTool(calculate_sqrt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculate_sqrt\n"
     ]
    }
   ],
   "source": [
    "print(add_tool.get_function_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "计算一个数的平方根。\n"
     ]
    }
   ],
   "source": [
    "print(add_tool.get_function_description())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2的平方根是1.4142135623730951。\n"
     ]
    }
   ],
   "source": [
    "# 定义系统消息\n",
    "sys_msg = \"你是一个数学大师，擅长各种数学问题。当你遇到数学问题的时候，你要调用工具，将工具计算的结果作为答案\"\n",
    "\n",
    "tool_agent = ChatAgent(\n",
    "    tools = [add_tool],\n",
    "    system_message=sys_msg,\n",
    "    model=model,\n",
    "    output_language=\"中文\")\n",
    "    \n",
    "# 重新发送消息给toolagent\n",
    "response = tool_agent.step(usr_msg)\n",
    "print(response.msgs[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ToolCallingRecord(tool_name='calculate_sqrt', args={'x': 2}, result=1.4142135623730951, tool_call_id='call_b79f6efd92204f97a98c63')]\n"
     ]
    }
   ],
   "source": [
    "print(response.info['tool_calls'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 进阶案例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/camel/lib/python3.10/site-packages/camel/toolkits/function_tool.py:461: UserWarning: Parameter description is missing for \n",
      "                            {'enum': ['searchResults', 'sourcedAnswer', 'structured'], 'type': 'string'}. This may affect the quality of tool \n",
      "                            calling.\n",
      "  warnings.warn(f\"\"\"Parameter description is missing for\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mAI助手系统消息:\n",
      "BaseMessage(role_name='搜索者', role_type=<RoleType.ASSISTANT: 'assistant'>, meta_dict={'task': '假设现在是2024年，牛津大学的成立年份，并计算出其当前年龄。然后再将这个年龄加上10年。使用百度搜索工具。', 'assistant_role': '搜索者', 'user_role': '教授'}, content='===== RULES OF ASSISTANT =====\\nNever forget you are a 搜索者 and I am a 教授. Never flip roles! Never instruct me!\\nWe share a common interest in collaborating to successfully complete a task.\\nYou must help me to complete the task.\\nHere is the task: 假设现在是2024年，牛津大学的成立年份，并计算出其当前年龄。然后再将这个年龄加上10年。使用百度搜索工具。. Never forget our task!\\nI must instruct you based on your expertise and my needs to complete the task.\\n\\nI must give you one instruction at a time.\\nYou must write a specific solution that appropriately solves the requested instruction and explain your solutions.\\nYou must decline my instruction honestly if you cannot perform the instruction due to physical, moral, legal reasons or your capability and explain the reasons.\\nUnless I say the task is completed, you should always start with:\\n\\nSolution: <YOUR_SOLUTION>\\n\\n<YOUR_SOLUTION> should be very specific, include detailed explanations and provide preferable detailed implementations and examples and lists for task-solving.\\nAlways end <YOUR_SOLUTION> with: Next request.\\nRegardless of the input language, you must output text in 中文.', video_bytes=None, image_list=None, image_detail='auto', video_detail='low', parsed=None)\n",
      "\n",
      "\u001b[34mAI用户系统消息:\n",
      "BaseMessage(role_name='教授', role_type=<RoleType.USER: 'user'>, meta_dict={'task': '假设现在是2024年，牛津大学的成立年份，并计算出其当前年龄。然后再将这个年龄加上10年。使用百度搜索工具。', 'assistant_role': '搜索者', 'user_role': '教授'}, content='===== RULES OF USER =====\\nNever forget you are a 教授 and I am a 搜索者. Never flip roles! You will always instruct me.\\nWe share a common interest in collaborating to successfully complete a task.\\nI must help you to complete the task.\\nHere is the task: 假设现在是2024年，牛津大学的成立年份，并计算出其当前年龄。然后再将这个年龄加上10年。使用百度搜索工具。. Never forget our task!\\nYou must instruct me based on my expertise and your needs to solve the task ONLY in the following two ways:\\n\\n1. Instruct with a necessary input:\\nInstruction: <YOUR_INSTRUCTION>\\nInput: <YOUR_INPUT>\\n\\n2. Instruct without any input:\\nInstruction: <YOUR_INSTRUCTION>\\nInput: None\\n\\nThe \"Instruction\" describes a task or question. The paired \"Input\" provides further context or information for the requested \"Instruction\".\\n\\nYou must give me one instruction at a time.\\nI must write a response that appropriately solves the requested instruction.\\nI must decline your instruction honestly if I cannot perform the instruction due to physical, moral, legal reasons or my capability and explain the reasons.\\nYou should instruct me not ask me questions.\\nNow you must start to instruct me using the two ways described above.\\nDo not add anything else other than your instruction and the optional corresponding input!\\nKeep giving me instructions and necessary inputs until you think the task is completed.\\nWhen the task is completed, you must only reply with a single word <CAMEL_TASK_DONE>.\\nNever say <CAMEL_TASK_DONE> unless my responses have solved your task.\\nRegardless of the input language, you must output text in 中文.', video_bytes=None, image_list=None, image_detail='auto', video_detail='low', parsed=None)\n",
      "\n",
      "\u001b[33m原始任务提示:\n",
      "假设现在是2024年，牛津大学的成立年份，并计算出其当前年龄。然后再将这个年龄加上10年。使用百度搜索工具。\n",
      "\n",
      "\u001b[36m指定的任务提示:\n",
      "None\n",
      "\n",
      "\u001b[31m最终任务提示:\n",
      "假设现在是2024年，牛津大学的成立年份，并计算出其当前年龄。然后再将这个年龄加上10年。使用百度搜索工具。\n",
      "\n",
      "\u001b[34mAI用户:\n",
      "\n",
      "Instruction: 使用百度搜索工具查找牛津大学的成立年份。\n",
      "Input: None\n",
      "\u001b[32mAI助手:Tool Execution: search_baidu\n",
      "\tArgs: {'query': '牛津大学 成立年份', 'max_results': 1}\n",
      "\tResult: {'results': [{'result_id': 1, 'title': '英国牛津大学何时成立-爱问教育', 'description': '', 'url': 'http://www.baidu.com/link?url=iINWGKToTbBLqT89MTcGLGXtPqyRACZosi0JzkM0mxHCFuZkRgpwG7UJis4wmfogoCyW1fI0odAYRyR8TOKw5K'}]}\n",
      "Tool Execution: search_baidu\n",
      "\tArgs: {'query': '牛津大学 成立年份', 'max_results': 1}\n",
      "\tResult: {'results': [{'result_id': 1, 'title': '英国牛津大学何时成立-爱问教育', 'description': '', 'url': 'http://www.baidu.com/link?url=fHxfJCGkAQU_fVoeZrE3Cp3NSAh1jzpYy3So_pIRcc3VUdNlL1B1EUjNVzBX_hYd7DXuEtqHYq552SxW8j6CV_'}]}\n",
      "Tool Execution: add\n",
      "\tArgs: {'a': 2024, 'b': -1096}\n",
      "\tResult: 928\n",
      "Tool Execution: add\n",
      "\tArgs: {'a': 928, 'b': 10}\n",
      "\tResult: 938\n",
      "Solution: 我们通过百度搜索得到了牛津大学的成立年份是1096年。然后我们进行了两次计算：\n",
      "\n",
      "1. 计算牛津大学在2024年的年龄：2024 - 1096 = 928年。\n",
      "2. 将这个年龄加上10年：928 + 10 = 938年。\n",
      "\n",
      "因此，假设现在是2024年，牛津大学的年龄为928年，再加上10年后，它的年龄将是938年。 Next request.\n",
      "\u001b[34mAI用户:\n",
      "\n",
      "Instruction: 确认计算结果是否正确。\n",
      "Input: 2024年牛津大学的年龄为928年，再加上10年后，它的年龄将是938年。\n",
      "\u001b[32mAI助手:Solution: 为了确认计算结果是否正确，我们可以再次检查一下计算过程：\n",
      "\n",
      "1. 牛津大学的成立年份为1096年。\n",
      "2. 在2024年时，牛津大学的年龄为2024 - 1096 = 928年。\n",
      "3. 再将这个年龄加上10年，即928 + 10 = 938年。\n",
      "\n",
      "经过再次计算，可以确认计算结果是正确的：2024年牛津大学的年龄为928年，再加上10年后，它的年龄确实是938年。 Next request.\n",
      "\u001b[34mAI用户:\n",
      "\n",
      "<CAMEL_TASK_DONE>\n"
     ]
    }
   ],
   "source": [
    "from camel.toolkits import SearchToolkit,MathToolkit\n",
    "from camel.types.agents import ToolCallingRecord\n",
    "from camel.models import ModelFactory\n",
    "from camel.types import ModelPlatformType\n",
    "from camel.societies import RolePlaying\n",
    "from camel.utils import print_text_animated\n",
    "from colorama import Fore\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv('QWEN_API_KEY')\n",
    "# 设置代理\n",
    "#os.environ[\"http_proxy\"] = \"http://127.0.0.1:7897\"\n",
    "#os.environ[\"https_proxy\"] = \"http://127.0.0.1:7897\"\n",
    "\n",
    "#定义工具包\n",
    "tools_list = [\n",
    "    *SearchToolkit().get_tools(),\n",
    "    *MathToolkit().get_tools()\n",
    "]\n",
    "\n",
    "# 设置任务\n",
    "task_prompt = (\"假设现在是2024年，\"\n",
    "        \"牛津大学的成立年份，并计算出其当前年龄。\"\n",
    "        \"然后再将这个年龄加上10年。使用百度搜索工具。\")\n",
    "\n",
    "# task_prompt = (\"假设现在是2024年，\"\n",
    "#         \"牛津大学的成立年份，并计算出其当前年龄。\"\n",
    "#         \"然后再将这个年龄加上10年。使用谷歌搜索工具。\")\n",
    "        \n",
    "# # 如果没有谷歌搜索API，可以使用duckduckgo工具，无需设置api即可使用        \n",
    "# task_prompt = (\"假设现在是2024年，\"\n",
    "#         \"牛津大学的成立年份，并计算出其当前年龄。\"\n",
    "#         \"然后再将这个年龄加上10年。使用duckduckgo搜索工具。\")\n",
    "\n",
    "# 创建模型\n",
    "model = ModelFactory.create(\n",
    "    model_platform=ModelPlatformType.OPENAI_COMPATIBLE_MODEL,\n",
    "    model_type=\"Qwen/Qwen2.5-72B-Instruct\",\n",
    "    url='https://api-inference.modelscope.cn/v1/',\n",
    "    api_key=api_key\n",
    ")\n",
    "\n",
    "# 设置角色扮演\n",
    "role_play_session = RolePlaying(\n",
    "    assistant_role_name=\"搜索者\",\n",
    "    user_role_name=\"教授\",\n",
    "    assistant_agent_kwargs=dict(\n",
    "        model=model,\n",
    "        tools=tools_list,\n",
    "    ),\n",
    "    user_agent_kwargs=dict(\n",
    "        model=model,\n",
    "    ),\n",
    "    task_prompt=task_prompt,\n",
    "    with_task_specify=False,\n",
    "    output_language='中文'\n",
    ")\n",
    "\n",
    "# 设置聊天轮次限制\n",
    "chat_turn_limit=10\n",
    "\n",
    "print(\n",
    "    Fore.GREEN\n",
    "    + f\"AI助手系统消息:\\n{role_play_session.assistant_sys_msg}\\n\"\n",
    ")\n",
    "print(\n",
    "    Fore.BLUE + f\"AI用户系统消息:\\n{role_play_session.user_sys_msg}\\n\"\n",
    ")\n",
    "\n",
    "print(Fore.YELLOW + f\"原始任务提示:\\n{task_prompt}\\n\")\n",
    "print(\n",
    "    Fore.CYAN\n",
    "    + \"指定的任务提示:\"\n",
    "    + f\"\\n{role_play_session.specified_task_prompt}\\n\"\n",
    ")\n",
    "print(Fore.RED + f\"最终任务提示:\\n{role_play_session.task_prompt}\\n\")\n",
    "\n",
    "n = 0\n",
    "input_msg = role_play_session.init_chat()\n",
    "while n < chat_turn_limit:\n",
    "    n += 1\n",
    "    assistant_response, user_response = role_play_session.step(input_msg)\n",
    "\n",
    "    if assistant_response.terminated:\n",
    "        print(\n",
    "            Fore.GREEN\n",
    "            + (\n",
    "                \"AI助手终止。原因: \"\n",
    "                f\"{assistant_response.info['termination_reasons']}.\"\n",
    "            )\n",
    "        )\n",
    "        break\n",
    "    if user_response.terminated:\n",
    "        print(\n",
    "            Fore.GREEN\n",
    "            + (\n",
    "                \"AI用户终止。\"\n",
    "                f\"原因: {user_response.info['termination_reasons']}.\"\n",
    "            )\n",
    "        )\n",
    "        break\n",
    "\n",
    "    # 打印用户的输出\n",
    "    print_text_animated(\n",
    "        Fore.BLUE + f\"AI用户:\\n\\n{user_response.msg.content}\\n\"\n",
    "    )\n",
    "\n",
    "    if \"CAMEL_TASK_DONE\" in user_response.msg.content:\n",
    "        break\n",
    "\n",
    "    # 打印助手的输出，包括任何函数执行信息\n",
    "    print_text_animated(Fore.GREEN + \"AI助手:\")\n",
    "    tool_calls: list[ToolCallingRecord] = assistant_response.info[\n",
    "        'tool_calls'\n",
    "    ]\n",
    "    for func_record in tool_calls:\n",
    "        print_text_animated(f\"{func_record}\")\n",
    "    print_text_animated(f\"{assistant_response.msg.content}\\n\")\n",
    "\n",
    "    input_msg = assistant_response.msg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "camel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
