{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c95a351b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from camel.societies import RolePlaying\n",
    "from camel.types import TaskType, ModelType, ModelPlatformType\n",
    "from camel.models import ModelFactory\n",
    "import os\n",
    "from camel.configs import ZhipuAIConfig\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "# 设置代理\n",
    "#os.environ[\"http_proxy\"] = \"http://127.0.0.1:7897\"\n",
    "#os.environ[\"https_proxy\"] = \"http://127.0.0.1:7897\"\n",
    "\n",
    "# model = ModelFactory.create(\n",
    "#     model_platform=ModelPlatformType.OPENAI_COMPATIBLE_MODEL,\n",
    "#     model_type=\"Qwen/Qwen2.5-72B-Instruct\",\n",
    "#     url='https://api-inference.modelscope.cn/v1/',\n",
    "#     api_key=os.getenv('ModelFactory_Key')\n",
    "# )\n",
    "model = ModelFactory.create(\n",
    "    model_platform=ModelPlatformType.ZHIPU,\n",
    "    model_type=ModelType.GLM_4,\n",
    "    model_config_dict=ZhipuAIConfig(temperature=0.2).as_dict(),\n",
    "    api_key=os.environ.get(\"ZHIPUAI_API_KEY\"),\n",
    "    url=os.environ.get(\"ZHIPUAI_API_BASE_URL\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "157aeae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from camel.agents import ChatAgent\n",
    "\n",
    "# agent = ChatAgent(model=model)\n",
    "# response = agent.step(\"你是谁?\")\n",
    "# response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73670e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#构建第一个agent-society\n",
    "\n",
    "#1.定义指令参数\n",
    "task_kwargs = {\n",
    "    'task_prompt': '制定一个计划去过去并进行改变。',\n",
    "    'with_task_specify': True,#开启后，将会有一个agent将我们的初始prompt进一步明确化\n",
    "    'task_specify_agent_kwargs': {'model': model}\n",
    "}\n",
    "#2.指令发送者参数\n",
    "user_role_kwargs = {\n",
    "    'user_role_name': '一个雄心勃勃的渴望成为时间旅行者的人',\n",
    "    'user_agent_kwargs': {'model': model}\n",
    "}\n",
    "#3.指令接收者参数\n",
    "assistant_role_kwargs = {\n",
    "    'assistant_role_name': '最优秀的实验物理学家',\n",
    "    'assistant_agent_kwargs': {'model': model}\n",
    "}\n",
    "\n",
    "society = RolePlaying(\n",
    "    **task_kwargs,             # 任务参数\n",
    "    **user_role_kwargs,        # 指令发送者的参数\n",
    "    **assistant_role_kwargs,   # 指令接收者的参数\n",
    "    critic_role_name='human',\n",
    "    with_critic_in_the_loop=True,\n",
    "    output_language=\"中文\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0395b23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义终止函数\n",
    "def is_terminated(response):\n",
    "    \"\"\"\n",
    "    判断响应结果是否终止\n",
    "    \"\"\"\n",
    "    if response.terminated:\n",
    "        role = response.msg.role_type.name()\n",
    "        reason = response.info['termination_reasons']\n",
    "        print(f\"[{role}] 终止对话，原因: {reason}\")\n",
    "    return response.terminated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91cac339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseMessage(role_name='最优秀的实验物理学家', role_type=<RoleType.ASSISTANT: 'assistant'>, meta_dict=None, content='Now start to give me instructions one by one. Only reply with Instruction and Input.', video_bytes=None, image_list=None, image_detail='auto', video_detail='low', parsed=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "society.init_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fc03976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m\n",
      "> Proposals from 一个雄心勃勃的渴望成为时间旅行者的人 (RoleType.USER). Please choose an option:\n",
      "\u001b[35m\u001b[3mOption 1:\n",
      "Instruction: 确定公元1361年朱元璋决战的具体时间和地点。\n",
      "Input: None\u001b[0m\n",
      "\u001b[35m\u001b[3mOption 2:\n",
      "Input by Kill Switch Engineer.\u001b[0m\n",
      "\u001b[35m\u001b[3mOption 3:\n",
      "Stop!!!\u001b[0m\n",
      "\n",
      "\n",
      "2025-09-01 07:40:18,270 - camel.models.model_manager - ERROR - Error processing with model: <camel.models.zhipuai_model.ZhipuAIModel object at 0x73870a2f9f60>\n",
      "2025-09-01 07:40:18,271 - camel.agents.chat_agent - ERROR - An error occurred while running model glm-4, index: 0\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspaces/handy-multi-agent/camel/camel/agents/chat_agent.py\", line 777, in _get_model_response\n",
      "    response = self.model_backend.run(\n",
      "  File \"/workspaces/handy-multi-agent/camel/camel/models/model_manager.py\", line 226, in run\n",
      "    raise exc\n",
      "  File \"/workspaces/handy-multi-agent/camel/camel/models/model_manager.py\", line 216, in run\n",
      "    response = self.current_model.run(messages, response_format, tools)\n",
      "  File \"/workspaces/handy-multi-agent/camel/camel/models/base_model.py\", line 50, in wrapped_run\n",
      "    return original_run(self, messages, *args, **kwargs)\n",
      "  File \"/workspaces/handy-multi-agent/camel/camel/models/base_model.py\", line 195, in run\n",
      "    return self._run(messages, response_format, tools)\n",
      "  File \"/workspaces/handy-multi-agent/camel/camel/models/zhipuai_model.py\", line 142, in _run\n",
      "    response = self._client.chat.completions.create(\n",
      "  File \"/opt/conda/envs/camel/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/camel/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1147, in create\n",
      "    return self._post(\n",
      "  File \"/opt/conda/envs/camel/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/opt/conda/envs/camel/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'code': '1213', 'message': '未正常接收到prompt参数。'}}\n"
     ]
    },
    {
     "ename": "ModelProcessingError",
     "evalue": "Unable to process messages: the only provided model did not run successfully. Error: Error code: 400 - {'error': {'code': '1213', 'message': '未正常接收到prompt参数。'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModelProcessingError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m         input_msg \u001b[38;5;241m=\u001b[39m assistant_response\u001b[38;5;241m.\u001b[39mmsg\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43msociety\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mround_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 9\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(society, round_limit)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#开始对话\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(round_limit):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m#获取这一轮的两个响应\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m     assistant_response, user_response \u001b[38;5;241m=\u001b[39m \u001b[43msociety\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_msg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m#检查终止条件\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_terminated(assistant_response) \u001b[38;5;129;01mor\u001b[39;00m is_terminated(user_response):\n",
      "File \u001b[0;32m/workspaces/handy-multi-agent/camel/camel/societies/role_playing.py:553\u001b[0m, in \u001b[0;36mRolePlaying.step\u001b[0;34m(self, assistant_msg)\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    548\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_agent\u001b[38;5;241m.\u001b[39mmodel_backend\u001b[38;5;241m.\u001b[39mmodel_config_dict\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_agent\u001b[38;5;241m.\u001b[39mmodel_backend\u001b[38;5;241m.\u001b[39mmodel_config_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    550\u001b[0m ):\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_agent\u001b[38;5;241m.\u001b[39mrecord_message(user_msg)\n\u001b[0;32m--> 553\u001b[0m assistant_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massistant_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_msg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m assistant_response\u001b[38;5;241m.\u001b[39mterminated \u001b[38;5;129;01mor\u001b[39;00m assistant_response\u001b[38;5;241m.\u001b[39mmsgs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    556\u001b[0m         ChatAgentResponse(\n\u001b[1;32m    557\u001b[0m             msgs\u001b[38;5;241m=\u001b[39m[],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    563\u001b[0m         ),\n\u001b[1;32m    564\u001b[0m     )\n",
      "File \u001b[0;32m/workspaces/handy-multi-agent/camel/camel/agents/chat_agent.py:595\u001b[0m, in \u001b[0;36mChatAgent.step\u001b[0;34m(self, input_message, response_format)\u001b[0m\n\u001b[1;32m    591\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step_token_exceed(\n\u001b[1;32m    592\u001b[0m         e\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m1\u001b[39m], tool_call_records, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens_exceeded\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    593\u001b[0m     )\n\u001b[1;32m    594\u001b[0m \u001b[38;5;66;03m# Get response from model backend\u001b[39;00m\n\u001b[0;32m--> 595\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_model_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopenai_messages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_full_tool_schemas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msingle_iteration:\n\u001b[1;32m    603\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/workspaces/handy-multi-agent/camel/camel/agents/chat_agent.py:795\u001b[0m, in \u001b[0;36mChatAgent._get_model_response\u001b[0;34m(self, openai_messages, num_tokens, response_format, tool_schemas)\u001b[0m\n\u001b[1;32m    790\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ModelProcessingError(\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to process messages: none of the provided models \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    792\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    793\u001b[0m     )\n\u001b[1;32m    794\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m response:\n\u001b[0;32m--> 795\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ModelProcessingError(\n\u001b[1;32m    796\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to process messages: the only provided model \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    797\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdid not run successfully. Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_info\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    798\u001b[0m     )\n\u001b[1;32m    800\u001b[0m sanitized_messages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sanitize_messages_for_logging(\n\u001b[1;32m    801\u001b[0m     openai_messages\n\u001b[1;32m    802\u001b[0m )\n\u001b[1;32m    803\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    804\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_backend\u001b[38;5;241m.\u001b[39mmodel_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_backend\u001b[38;5;241m.\u001b[39mcurrent_model_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    806\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocessed these messages: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msanitized_messages\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    807\u001b[0m )\n",
      "\u001b[0;31mModelProcessingError\u001b[0m: Unable to process messages: the only provided model did not run successfully. Error: Error code: 400 - {'error': {'code': '1213', 'message': '未正常接收到prompt参数。'}}"
     ]
    }
   ],
   "source": [
    "#开始对话\n",
    "def run(society, round_limit: int=3):\n",
    "    #获取AI助手到AI用户的初始消息\n",
    "    input_msg = society.init_chat()\n",
    "\n",
    "    #开始对话\n",
    "    for _ in range(round_limit):\n",
    "        #获取这一轮的两个响应\n",
    "        assistant_response, user_response = society.step(input_msg)\n",
    "        \n",
    "        #检查终止条件\n",
    "        if is_terminated(assistant_response) or is_terminated(user_response):\n",
    "            break\n",
    "        # 获取结果\n",
    "        print(f'[AI用户]: {user_response.msg.content}.\\n')\n",
    "        #检查任务是否结束\n",
    "        if 'CAMEL_TASK_DONE' in user_response.msg.content:\n",
    "            print(\"任务完成！\")\n",
    "            break\n",
    "        print(f'[AI助手]: {assistant_response.msg.content}.\\n')\n",
    "\n",
    "        #获取下一轮的输入消息\n",
    "        input_msg = assistant_response.msg\n",
    "    return None\n",
    "\n",
    "run(society, round_limit=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a58adab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "camel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
